{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Packages"
      ],
      "metadata": {
        "id": "8qWQy_PMQ-t8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X20ERLBTSVZk",
        "outputId": "1cd2a54f-3369-4072-b1c4-47a4cbbff61a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.169.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.24.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.4.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-api-python-client\n",
        "!pip install networkx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import json\n",
        "import networkx as nx\n",
        "import os\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "YRIhYu2gEB57"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "QH7HWMPRQ4y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_videos(query, max_results):\n",
        "    videos = []\n",
        "    next_page_token = None\n",
        "\n",
        "    while len(videos) < max_results:\n",
        "        remaining = max_results - len(videos)\n",
        "        fetch_count = min(50, remaining)\n",
        "\n",
        "        search_response = youtube.search().list(\n",
        "            q=query,\n",
        "            part='snippet',\n",
        "            type='video',\n",
        "            maxResults=fetch_count,\n",
        "            pageToken=next_page_token\n",
        "        ).execute()\n",
        "\n",
        "        for item in search_response['items']:\n",
        "            video_info = {\n",
        "                'id': item['id']['videoId'],\n",
        "                'title': item['snippet']['title'],\n",
        "                'publishedAt': item['snippet']['publishedAt'],\n",
        "                'channelId': item['snippet']['channelId'],\n",
        "                'author': item['snippet']['channelTitle']\n",
        "            }\n",
        "            videos.append(video_info)\n",
        "\n",
        "        next_page_token = search_response.get('nextPageToken')\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return videos[:max_results]"
      ],
      "metadata": {
        "id": "nudrFx_sSiuz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_comments(video_id):\n",
        "    comments = []\n",
        "\n",
        "    def get_page(page_token=None):\n",
        "        request = youtube.commentThreads().list(\n",
        "            part='snippet,replies',\n",
        "            videoId=video_id,\n",
        "            maxResults=100,\n",
        "            pageToken=page_token,\n",
        "            textFormat='plainText'\n",
        "        )\n",
        "        return request.execute()\n",
        "\n",
        "    response = get_page()\n",
        "    while response:\n",
        "        for item in response['items']:\n",
        "            top_comment = item['snippet']['topLevelComment']['snippet']\n",
        "            comment_data = {\n",
        "                'commentId': item['id'],\n",
        "                'text': top_comment['textDisplay'],\n",
        "                'author': top_comment['authorDisplayName'],\n",
        "                'publishedAt': top_comment['publishedAt'],\n",
        "                'replies': []\n",
        "            }\n",
        "\n",
        "            if 'replies' in item:\n",
        "                for reply in item['replies']['comments']:\n",
        "                    reply_snippet = reply['snippet']\n",
        "                    comment_data['replies'].append({\n",
        "                        'commentId': reply['id'],\n",
        "                        'text': reply_snippet['textDisplay'],\n",
        "                        'author': reply_snippet['authorDisplayName'],\n",
        "                        'publishedAt': reply_snippet['publishedAt'],\n",
        "                    })\n",
        "\n",
        "            comments.append(comment_data)\n",
        "\n",
        "        if 'nextPageToken' in response:\n",
        "            response = get_page(response['nextPageToken'])\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return comments\n"
      ],
      "metadata": {
        "id": "RuAeG30oSme0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_comment_tree(comments):\n",
        "    for c in comments:\n",
        "        print(f\"{c['author']} said at {c['publishedAt']}: {c['text']}\")\n",
        "        for r in c['replies']:\n",
        "            print(f\"  â†³ {r['author']} replied at {r['publishedAt']}: {r['text']}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "dcCbG5K9SsjN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_comment_tree_as_json(comments, filename='comment_tree.json'):\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(comments, f, ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "MMX-D67xT4_u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_replies_recursively(parent_id):\n",
        "    replies = []\n",
        "    next_page_token = None\n",
        "\n",
        "    while True:\n",
        "        request = youtube.comments().list(\n",
        "            part='snippet',\n",
        "            parentId=parent_id,\n",
        "            textFormat='plainText',\n",
        "            maxResults=100,\n",
        "            pageToken=next_page_token\n",
        "        )\n",
        "        response = request.execute()\n",
        "\n",
        "        for item in response.get('items', []):\n",
        "            reply_snippet = item['snippet']\n",
        "            reply_data = {\n",
        "                'commentId': item['id'],\n",
        "                'text': reply_snippet['textDisplay'],\n",
        "                'author': reply_snippet['authorDisplayName'],\n",
        "                'publishedAt': reply_snippet['publishedAt'],\n",
        "                'replies': fetch_replies_recursively(item['id'])  # ðŸ§  recursion here!\n",
        "            }\n",
        "            replies.append(reply_data)\n",
        "\n",
        "        next_page_token = response.get('nextPageToken')\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return replies\n"
      ],
      "metadata": {
        "id": "Ht2abwlXUloE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_full_comment_tree(video_id):\n",
        "    comments = []\n",
        "    next_page_token = None\n",
        "\n",
        "    print(API_KEY)\n",
        "\n",
        "    while True:\n",
        "        request = youtube.commentThreads().list(\n",
        "            part='snippet',\n",
        "            videoId=video_id,\n",
        "            maxResults=100,\n",
        "            textFormat='plainText',\n",
        "            pageToken=next_page_token\n",
        "        )\n",
        "        response = request.execute()\n",
        "\n",
        "        for item in response.get('items', []):\n",
        "            top_comment_snippet = item['snippet']['topLevelComment']['snippet']\n",
        "            comment_data = {\n",
        "                'commentId': item['id'],\n",
        "                'text': top_comment_snippet['textDisplay'],\n",
        "                'author': top_comment_snippet['authorDisplayName'],\n",
        "                'publishedAt': top_comment_snippet['publishedAt'],\n",
        "                'replies': fetch_replies_recursively(item['id'])\n",
        "            }\n",
        "            comments.append(comment_data)\n",
        "\n",
        "        next_page_token = response.get('nextPageToken')\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return comments\n"
      ],
      "metadata": {
        "id": "dJpZGiI0V-N-"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_depth(comments): #Not necessary depth is fixed at 2 in Youtube\n",
        "    if not comments:\n",
        "        return 0\n",
        "    return 1 + max((get_max_depth(comment.get('replies', [])) for comment in comments), default=0)\n"
      ],
      "metadata": {
        "id": "7pZitt3fWaZQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query"
      ],
      "metadata": {
        "id": "fvLcFHeHQzPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY=''\n",
        "query='parking bonaire valencia'\n",
        "os.makedirs(query, exist_ok=True)\n",
        "results_number=100"
      ],
      "metadata": {
        "id": "AbvjKmReENWT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
        "videos = search_videos('parking bonaire valencia', max_results=results_number)\n",
        "with open(query+'/videos_data.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(videos, f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "r78ejpbBE9Lv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comment Trees"
      ],
      "metadata": {
        "id": "OwTYgZDTQpIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY=''\n",
        "query='parking bonaire valencia'\n",
        "results_number=100\n",
        "\n",
        "with open(query+'/videos_data.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "video_ids = [video['id'] for video in data]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQCbxI-FnbU4",
        "outputId": "38a0e8d8-f6a0-4ab3-fd3d-de8472677ea5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 ['p2JaYHVnJiA', '6BMKx4sQ2o8', '3nrMNFtgKOo', 'LRtSPJi4Byk', 'u4Tnc5Z3u-Y', '5Ml8j7L_4c8', 'IpnKlUAzkoQ', 'dP-fovsQUYc', '-1xTSrm3p_0', 'OaseDDoc4ZQ', 'JRheS8fDrWM', 'dgSM5iDeHzU', 'bLx6AErC2nQ', 'nfVyFlqr3Go', 'SSZfvpx4grQ', 'ivnS8ZE3AO4', 'YnDCuawdEDM', 'Fy6GnmVzChM', 'zzIzMlbZu3s', 'sKRRt855AwI', 'aPouPRfDNA4', 'rYEJUMFt0BI', '1oz3ndF76WA', 'mOnaqqncMx4', '-tzCe4R5beg', 'UozH6vKj_GU', 'LS1jakQlCQ8', 'qk9IjnHodmM', 'Jo6ZOV8NS3g', 'CHBrNCP1BIs', 'MZDtnUxTbUI', 'NOXfLILDlB0', 'n5PM0OWhHNY', 'FwRsnP2BEdI', 'IByXtKzZusE', '7cEX45Mstak', 'gx-46d9NpNY', '0_o_p_GjFD0', 'eSalGRSXG4Q', 'OaKyLCJ75WU', 's21f1Q_kyA0', 'd7Ybk89ZKVo', 'J1NGIwkRApA', '9l7rUQtW-9Y', '574Q_dyRNZM', 'krYkQ9Z6A1Y', 'GkRmMcMk8U0', 'aUZBuQJ3pKQ', 'BaCW1W2cU5g', 'DWvjDi1eDNQ', 'aUZBuQJ3pKQ', '3eC9MIYGsbI', 'UC0fAr1OGHk', 'Y51xfO35ENk', 'GosHhphzLTo', 'SzuClHgS0Ow', '9bcJt3mRCIo', 'py4Y0ac-0ks', 'cbYVznlbWqk', 'SJ576J3uULc', 'ta1W5lScquI', 'jVQ9PY6nmKc', 'mLqFm4gIuAc', '8nWSEWgaGlA', '6psSXZ0vReA', 'QKauHor_2Jw', 'yXKblUPp3mU', 'JLWNaCWNJqE', 'ZGJpAM7fL3Q', 'hn9IpLpbkTA', 'LZwOY25pfoA', 'sxSJ-sWWlBw', '2esskDJ8E_o', 'BU-5kuF0y-E', 'L5lXyh98Itg', 'iUcnGXnQS6s', 'EXEDXMjE6-U', 'GBCNh5W229Y', 'SzB0kxkr5gU', 'lbXx9zAhg48', 'PoNSUbTCXE4', 'An2n874v-W8', 'T7D-LlLFpDg', 'kWHWO4NCWac', 'IqAPFfIXTVQ', 'gPYtAfkl324', 'Oi0G7Lk80xE', 'xQv4BeP6kmY', 'sVjQzk7UpgA', 'HUtqIb7fc94', '84NorZ4KFiw', '5OGUwBDItGc', 'MhPmA78ysVU', 'ricGhCxX69s', 'IFZ2EZcLsEg', 'Npv7b2RRET0', 'Bp6UDhqeEe4', 'vSI2Frq5aoA', 'fZlY_3NK45Y', 'fha4HWsflnM']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_folder = os.path.join(query,'comment_trees')\n",
        "json_files = [f for f in os.listdir(output_folder) if f.endswith('.json')]\n",
        "json_ids = {os.path.splitext(f)[0] for f in json_files}\n",
        "remaining_ids = [id_ for id_ in video_ids if id_ not in json_ids]\n",
        "\n",
        "print(len(video_ids),video_ids)\n",
        "print(len(json_ids),json_ids)\n",
        "print(len(remaining_ids), remaining_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc6wRgITol9R",
        "outputId": "2372f2fd-ae6e-40c4-bcf9-f332854fe385"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 ['p2JaYHVnJiA', '6BMKx4sQ2o8', '3nrMNFtgKOo', 'LRtSPJi4Byk', 'u4Tnc5Z3u-Y', '5Ml8j7L_4c8', 'IpnKlUAzkoQ', 'dP-fovsQUYc', '-1xTSrm3p_0', 'OaseDDoc4ZQ', 'JRheS8fDrWM', 'dgSM5iDeHzU', 'bLx6AErC2nQ', 'nfVyFlqr3Go', 'SSZfvpx4grQ', 'ivnS8ZE3AO4', 'YnDCuawdEDM', 'Fy6GnmVzChM', 'zzIzMlbZu3s', 'sKRRt855AwI', 'aPouPRfDNA4', 'rYEJUMFt0BI', '1oz3ndF76WA', 'mOnaqqncMx4', '-tzCe4R5beg', 'UozH6vKj_GU', 'LS1jakQlCQ8', 'qk9IjnHodmM', 'Jo6ZOV8NS3g', 'CHBrNCP1BIs', 'MZDtnUxTbUI', 'NOXfLILDlB0', 'n5PM0OWhHNY', 'FwRsnP2BEdI', 'IByXtKzZusE', '7cEX45Mstak', 'gx-46d9NpNY', '0_o_p_GjFD0', 'eSalGRSXG4Q', 'OaKyLCJ75WU', 's21f1Q_kyA0', 'd7Ybk89ZKVo', 'J1NGIwkRApA', '9l7rUQtW-9Y', '574Q_dyRNZM', 'krYkQ9Z6A1Y', 'GkRmMcMk8U0', 'aUZBuQJ3pKQ', 'BaCW1W2cU5g', 'DWvjDi1eDNQ', 'aUZBuQJ3pKQ', '3eC9MIYGsbI', 'UC0fAr1OGHk', 'Y51xfO35ENk', 'GosHhphzLTo', 'SzuClHgS0Ow', '9bcJt3mRCIo', 'py4Y0ac-0ks', 'cbYVznlbWqk', 'SJ576J3uULc', 'ta1W5lScquI', 'jVQ9PY6nmKc', 'mLqFm4gIuAc', '8nWSEWgaGlA', '6psSXZ0vReA', 'QKauHor_2Jw', 'yXKblUPp3mU', 'JLWNaCWNJqE', 'ZGJpAM7fL3Q', 'hn9IpLpbkTA', 'LZwOY25pfoA', 'sxSJ-sWWlBw', '2esskDJ8E_o', 'BU-5kuF0y-E', 'L5lXyh98Itg', 'iUcnGXnQS6s', 'EXEDXMjE6-U', 'GBCNh5W229Y', 'SzB0kxkr5gU', 'lbXx9zAhg48', 'PoNSUbTCXE4', 'An2n874v-W8', 'T7D-LlLFpDg', 'kWHWO4NCWac', 'IqAPFfIXTVQ', 'gPYtAfkl324', 'Oi0G7Lk80xE', 'xQv4BeP6kmY', 'sVjQzk7UpgA', 'HUtqIb7fc94', '84NorZ4KFiw', '5OGUwBDItGc', 'MhPmA78ysVU', 'ricGhCxX69s', 'IFZ2EZcLsEg', 'Npv7b2RRET0', 'Bp6UDhqeEe4', 'vSI2Frq5aoA', 'fZlY_3NK45Y', 'fha4HWsflnM']\n",
            "28 {'dP-fovsQUYc', 'IpnKlUAzkoQ', 'YnDCuawdEDM', '-1xTSrm3p_0', 'LRtSPJi4Byk', 'OaseDDoc4ZQ', '5Ml8j7L_4c8', 'eSalGRSXG4Q', 'Fy6GnmVzChM', 'mOnaqqncMx4', 'JRheS8fDrWM', '-tzCe4R5beg', 'ivnS8ZE3AO4', 'zzIzMlbZu3s', 'nfVyFlqr3Go', 'UozH6vKj_GU', 'bLx6AErC2nQ', 'rYEJUMFt0BI', '0_o_p_GjFD0', '6BMKx4sQ2o8', 'p2JaYHVnJiA', 'dgSM5iDeHzU', 'sKRRt855AwI', 'u4Tnc5Z3u-Y', 'SSZfvpx4grQ', '1oz3ndF76WA', 'aPouPRfDNA4', '3nrMNFtgKOo'}\n",
            "72 ['LS1jakQlCQ8', 'qk9IjnHodmM', 'Jo6ZOV8NS3g', 'CHBrNCP1BIs', 'MZDtnUxTbUI', 'NOXfLILDlB0', 'n5PM0OWhHNY', 'FwRsnP2BEdI', 'IByXtKzZusE', '7cEX45Mstak', 'gx-46d9NpNY', 'OaKyLCJ75WU', 's21f1Q_kyA0', 'd7Ybk89ZKVo', 'J1NGIwkRApA', '9l7rUQtW-9Y', '574Q_dyRNZM', 'krYkQ9Z6A1Y', 'GkRmMcMk8U0', 'aUZBuQJ3pKQ', 'BaCW1W2cU5g', 'DWvjDi1eDNQ', 'aUZBuQJ3pKQ', '3eC9MIYGsbI', 'UC0fAr1OGHk', 'Y51xfO35ENk', 'GosHhphzLTo', 'SzuClHgS0Ow', '9bcJt3mRCIo', 'py4Y0ac-0ks', 'cbYVznlbWqk', 'SJ576J3uULc', 'ta1W5lScquI', 'jVQ9PY6nmKc', 'mLqFm4gIuAc', '8nWSEWgaGlA', '6psSXZ0vReA', 'QKauHor_2Jw', 'yXKblUPp3mU', 'JLWNaCWNJqE', 'ZGJpAM7fL3Q', 'hn9IpLpbkTA', 'LZwOY25pfoA', 'sxSJ-sWWlBw', '2esskDJ8E_o', 'BU-5kuF0y-E', 'L5lXyh98Itg', 'iUcnGXnQS6s', 'EXEDXMjE6-U', 'GBCNh5W229Y', 'SzB0kxkr5gU', 'lbXx9zAhg48', 'PoNSUbTCXE4', 'An2n874v-W8', 'T7D-LlLFpDg', 'kWHWO4NCWac', 'IqAPFfIXTVQ', 'gPYtAfkl324', 'Oi0G7Lk80xE', 'xQv4BeP6kmY', 'sVjQzk7UpgA', 'HUtqIb7fc94', '84NorZ4KFiw', '5OGUwBDItGc', 'MhPmA78ysVU', 'ricGhCxX69s', 'IFZ2EZcLsEg', 'Npv7b2RRET0', 'Bp6UDhqeEe4', 'vSI2Frq5aoA', 'fZlY_3NK45Y', 'fha4HWsflnM']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY=''\n",
        "for element in remaining_ids:\n",
        "  os.makedirs(output_folder, exist_ok=True)\n",
        "  comment_tree =get_full_comment_tree(element)\n",
        "  save_comment_tree_as_json(comment_tree, filename=output_folder+'/'+element+'.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "yhwKwwPcFO8X",
        "outputId": "61b3c8e5-5eb6-4e86-ef2c-5a0e78ca44a0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HttpError",
          "evalue": "<HttpError 400 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=LS1jakQlCQ8&maxResults=100&textFormat=plainText&key=AIzaSyC3_WXUjKM91a_5hQYeepHigLFUe7j72z4&alt=json returned \"API key expired. Please renew the API key.\". Details: \"[{'message': 'API key expired. Please renew the API key.', 'domain': 'global', 'reason': 'badRequest'}]\">",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-1ceecaf35b6f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mremaining_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mcomment_tree\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mget_full_comment_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0msave_comment_tree_as_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-ecfc6f1e8bc0>\u001b[0m in \u001b[0;36mget_full_comment_tree\u001b[0;34m(video_id)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mpageToken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext_page_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         )\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'items'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHttpError\u001b[0m: <HttpError 400 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=LS1jakQlCQ8&maxResults=100&textFormat=plainText&key=AIzaSyC3_WXUjKM91a_5hQYeepHigLFUe7j72z4&alt=json returned \"API key expired. Please renew the API key.\". Details: \"[{'message': 'API key expired. Please renew the API key.', 'domain': 'global', 'reason': 'badRequest'}]\">"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graphs"
      ],
      "metadata": {
        "id": "oIdqNCMmQgyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Undirected Bipartite graph generation where all authors are connected to the\n",
        "# videos they have commented or responded on\n",
        "B = nx.Graph()\n",
        "\n",
        "authors = set()\n",
        "edges =set()\n",
        "input_path=os.path.join(query,'comment_trees')\n",
        "\n",
        "for filename in os.listdir(input_path):\n",
        "  if filename.endswith('.json'):\n",
        "    video_id=filename[-16:-5]\n",
        "    file_path = os.path.join(input_path, filename)\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "      try:\n",
        "        comments = json.load(file)\n",
        "        for comment in comments:\n",
        "          authors.add(comment['author'])\n",
        "          edges.add((video_id,comment['author']))\n",
        "\n",
        "          for reply in comment['replies']:\n",
        "            authors.add(reply['author'])\n",
        "            edges.add((video_id,reply['author']))\n",
        "      except json.JSONDecodeError:\n",
        "                    print(f\"Error decoding JSON in file {filename}\")\n",
        "\n",
        "B.add_nodes_from(video_ids, bipartite=0)\n",
        "B.add_nodes_from(authors, bipartite=1)\n",
        "\n",
        "B.add_edges_from(edges)\n",
        "nx.write_graphml(B, query+\"/graph_1.graphml\")"
      ],
      "metadata": {
        "id": "nKhUv55yKfsK"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directed bipartite graph generation where all authors are connected to the\n",
        "# videos they on and to the other users they have responded to with strength\n",
        "# equal to the number of comments/responses to that particular video/author\n",
        "G = nx.DiGraph()\n",
        "\n",
        "authors = set()\n",
        "video_ids = set()\n",
        "\n",
        "edge_type1_counts = defaultdict(int)\n",
        "edge_type2_counts = defaultdict(int)\n",
        "edge_type3_counts = defaultdict(int)\n",
        "\n",
        "input_path = os.path.join(query, 'comment_trees')\n",
        "\n",
        "for filename in os.listdir(input_path):\n",
        "    if filename.endswith('.json'):\n",
        "        video_id = filename[-16:-5]\n",
        "        video_ids.add(video_id)\n",
        "        file_path = os.path.join(input_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            try:\n",
        "                comments = json.load(file)\n",
        "                for comment in comments:\n",
        "                    author = comment['author']\n",
        "                    authors.add(author)\n",
        "\n",
        "                    edge_type1_counts[(author, video_id)] += 1\n",
        "\n",
        "                    for reply in comment.get('replies', []):\n",
        "                        replier = reply['author']\n",
        "                        authors.add(replier)\n",
        "\n",
        "                        edge_type2_counts[(replier, video_id)] += 1\n",
        "\n",
        "                        edge_type3_counts[(replier, author)] += 1\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Error decoding JSON in file {filename}\")\n",
        "\n",
        "# Add nodes\n",
        "G.add_nodes_from(video_ids, type='video')\n",
        "G.add_nodes_from(authors, type='author')\n",
        "\n",
        "# Add Type 1 edges (non-directed, but we add them as edges with attribute 'type')\n",
        "for (author, video), count in edge_type1_counts.items():\n",
        "    G.add_edge(author, video, type='Comment', strength=count)\n",
        "\n",
        "# Add Type 2 edges (directed)\n",
        "for (replier, commenter), count in edge_type2_counts.items():\n",
        "    G.add_edge(replier, commenter, type='Reply', strength=count)\n",
        "\n",
        "\n",
        "# Add Type 3 edges (directed)\n",
        "#for (replier, video), count in edge_type3_counts.items():\n",
        "#    G.add_edge(replier, video, type='type3', strength=count)\n",
        "\n",
        "nx.write_graphml(G, query+\"/graph_2.graphml\")"
      ],
      "metadata": {
        "id": "zRDmicGZP5mI"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download"
      ],
      "metadata": {
        "id": "Yod5in63QVvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compress and download query\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive(query, 'zip', query)\n",
        "files.download(query+'.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "cqzFEZ4iMgCL",
        "outputId": "9390bbc5-7cd0-4b70-eacf-08ad78198e66"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5535c33b-1102-48a9-be12-0599700d171a\", \"parking bonaire valencia.zip\", 2138312)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}